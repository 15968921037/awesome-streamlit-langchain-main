############################################
import streamlit as st
from langchain.llms import OpenAI
from langchain.vectorstores import Chroma
from sentence_transformers import SentenceTransformer
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.schema import AIMessage, HumanMessage
import os
from dotenv import load_dotenv  # å¯¼å…¥ dotenv åº“
from langchain_core.tools import tool
from sympy import sympify, symbols, Eq, solve
from scipy.stats import norm
import functools
import operator
from langchain_core.messages import BaseMessage, HumanMessage  # å¯¼å…¥æ¶ˆæ¯ç›¸å…³çš„ç±»
from langchain_openai.chat_models import ChatOpenAI  # å¯¼å…¥ OpenAI çš„èŠå¤©æ¨¡å‹ç±»
from langgraph.prebuilt import create_react_agent  # å¯¼å…¥åˆ›å»º React é£æ ¼ä»£ç†çš„å‡½æ•°
from langgraph.graph import END, StateGraph, START  # ç”¨äºå›¾å½¢çŠ¶æ€ç®¡ç†
from langchain_core.messages import HumanMessage  # ç”¨äºåˆ›å»ºç”¨æˆ·æ¶ˆæ¯
#Define State å®šä¹‰çŠ¶æ€
from typing import Annotated, Sequence, TypedDict
from langchain_openai import ChatOpenAI
from langchain.llms import OpenAI
from langchain.vectorstores import Chroma
from sentence_transformers import SentenceTransformer
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.schema import AIMessage, HumanMessage
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.tools.tavily_search import TavilySearchResults


# å¯¼å…¥æ‰€éœ€æ¨¡å—
from typing import List, Optional  # ç”¨äºç±»å‹æ³¨è§£ï¼ŒListè¡¨ç¤ºåˆ—è¡¨ï¼ŒOptionalè¡¨ç¤ºå¯é€‰ç±»å‹
from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser  # ç”¨äºè§£æ OpenAI å‡½æ•°è°ƒç”¨çš„è¾“å‡º
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  # ç”¨äºåˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿å’Œå ä½ç¬¦
from langchain_openai import ChatOpenAI  # ç”¨äºä¸ OpenAI çš„èŠå¤©æ¥å£äº¤äº’



# --- åŠ è½½ .env æ–‡ä»¶ä¸­çš„å˜é‡ ---
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
api_url = os.getenv("OPENAI_API_URL")

st.title("ğŸ“š Financial Academic Knowledge")

llm = ChatOpenAI(model="gpt-4o", base_url=api_url, api_key=api_key, temperature=0)

###################local
# --- Configure Sentence Transformer Model ---
@st.cache_resource
def load_embedding_model():
    """Load the embedding model."""
    return SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

model = load_embedding_model()

def embedding_function(text):
    """Convert text to embedding using the SentenceTransformer model."""
    return model.encode([text])[0].tolist()

# --- Load Chroma Vectorstore ---
@st.cache_resource
def load_vectorstore():
    """Load the Chroma vector database."""
    return Chroma(
        persist_directory="data/chroma_storage",
        embedding_function=embedding_function,
    )

vectorstore = load_vectorstore()

@tool
def local_knowledge_search(query: str) -> str:
    """
    åœ¨æœ¬åœ°çŸ¥è¯†åº“ä¸­æœç´¢é—®é¢˜çš„ç­”æ¡ˆã€‚

    å‚æ•°:
    - query: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜ã€‚

    è¿”å›:
    - ä»æœ¬åœ°çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„å†…å®¹ã€‚
    
    """
    
    RAG_TEMPLATE = """
    You are a financial Q&A assistant. Please answer the following question based on the provided context.
    Summarize with your own understanding to ensure smooth sentencesã€‚
    Make sure the explanation is simple enough for someone without a financial background to understand.
    If you are unable to answer, clearly state "I don't know". Ensure your answer is detailed, accurateã€‚

    <context>
    {context}
    </context>

    Answer the following question:

    {question}"""

    rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)


    chain = (
        RunnablePassthrough.assign(context=lambda input: format_docs(input["context"]))
        | rag_prompt
        | llm
        | StrOutputParser()
    )

    question_embedding = embedding_function(query)  # ç”ŸæˆåµŒå…¥
    docs = vectorstore.similarity_search_by_vector(question_embedding)  # æ‰§è¡Œæ£€ç´¢
    if not docs:
#         return "No relevant information found in the local knowledge base."
          return "I don't know"
    return "\n\n".join(doc.page_content for doc in docs)



# --- å®šä¹‰ scrape_webpages å·¥å…· ---
@tool
def scrape_webpages(urls: List[str]) -> str:
    """
    ä½¿ç”¨ WebBaseLoader æŠ“å–æŒ‡å®šç½‘é¡µå†…å®¹å¹¶è¿”å›ã€‚
    å‚æ•°:
    - urls: ä¸€ä¸ªåŒ…å«ç½‘é¡µ URL çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚

    è¿”å›:
    - ä¸€ä¸ªåŒ…å«æ‰€æœ‰æŠ“å–åˆ°ç½‘é¡µå†…å®¹çš„å­—ç¬¦ä¸²ï¼Œæ¯ä¸ªç½‘é¡µå†…å®¹ä¹‹é—´ç”¨ä¸¤ä¸ªæ¢è¡Œç¬¦åˆ†éš”ã€‚
    """
    loader = WebBaseLoader(urls)
    docs = loader.load()
    return "\n\n".join(
        [f'\n{doc.page_content}\n' for doc in docs]  # æ‹¼æ¥æ–‡æ¡£å†…å®¹
    )





# å®šä¹‰ä¸€ä¸ªå›¢é˜ŸçŠ¶æ€ç±»ï¼Œç”¨äºè·Ÿè¸ªå›¢é˜Ÿæˆå‘˜å’Œä»»åŠ¡çŠ¶æ€
class CalcuTeamState(TypedDict):
    """
    ResearchTeamState ç”¨äºè¡¨ç¤ºå›¢é˜Ÿæˆå‘˜çŠ¶æ€çš„å­—å…¸ç»“æ„ã€‚æ¯ä¸ªå›¢é˜Ÿæˆå‘˜çš„æ¶ˆæ¯ä¼šè¢«è®°å½•ä¸‹æ¥ï¼Œ
    å¹¶ä¸”é€šè¿‡â€œnextâ€å­—æ®µæ¥å†³å®šä¸‹ä¸€ä¸ªåº”è¯¥æ‰§è¡Œä»»åŠ¡çš„æˆå‘˜ã€‚
    """
    # æ¯ä¸ªå›¢é˜Ÿæˆå‘˜å®Œæˆä»»åŠ¡åä¼šç”Ÿæˆä¸€æ¡æ¶ˆæ¯
    messages: Annotated[List[BaseMessage], operator.add]  # æ¶ˆæ¯åˆ—è¡¨ï¼Œè®°å½•å›¢é˜Ÿæˆå‘˜çš„æ‰€æœ‰è¾“å‡º
    # å›¢é˜Ÿæˆå‘˜çš„æŠ€èƒ½ä¿¡æ¯ï¼Œä»¥ä¾¿æ¯ä¸ªæˆå‘˜äº†è§£å…¶ä»–æˆå‘˜çš„èƒ½åŠ›
    team_members: List[str]  # å›¢é˜Ÿæˆå‘˜åˆ—è¡¨
    # ç”¨æ¥è·¯ç”±ä»»åŠ¡ï¼Œç›‘ç£è€…ä¼šæ ¹æ®å½“å‰çŠ¶æ€å†³å®šå“ªä¸ªæˆå‘˜ç»§ç»­æ‰§è¡Œä»»åŠ¡
    next: str  # å½“å‰è½®åˆ°çš„æˆå‘˜ï¼Œæˆ–è€…â€œFINISHâ€è¡¨ç¤ºä»»åŠ¡å®Œæˆ

# å®šä¹‰ä»£ç†èŠ‚ç‚¹çš„å‡½æ•°ï¼Œç”¨äºè°ƒç”¨å…·ä½“çš„ä»£ç†å¹¶è¿”å›ç»“æœ
def agent_node(state, agent, name):
    result = agent.invoke(state)  # è°ƒç”¨ä»£ç†ï¼Œä¼ å…¥çŠ¶æ€
    return {"messages": [HumanMessage(content=result["messages"][-1].content, name=name)]}  # è¿”å›ä»£ç†ç»“æœ

# åˆ›å»ºä¸€ä¸ªå›¢é˜Ÿç›‘ç£è€…å‡½æ•°ï¼Œä½œä¸ºåŸºäºLLMçš„è·¯ç”±å™¨
def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:
    """
    åˆ›å»ºä¸€ä¸ªåŸºäº LLM çš„å›¢é˜Ÿè·¯ç”±å™¨ï¼Œè´Ÿè´£é€‰æ‹©ä¸‹ä¸€ä¸ªéœ€è¦æ‰§è¡Œä»»åŠ¡çš„è§’è‰²ã€‚
    å‚æ•°:
    - llm: ä½¿ç”¨çš„èŠå¤©æ¨¡å‹ï¼Œé€šå¸¸ä¸º ChatOpenAI ç±»å‹
    - system_prompt: ç³»ç»Ÿæç¤ºï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹çš„è¡Œä¸º
    - members: ä»»åŠ¡å›¢é˜Ÿçš„æˆå‘˜åˆ—è¡¨
    
    è¿”å›:
    - è¿”å›åŒ…å«è·¯ç”±é€»è¾‘å’Œè¾“å‡ºè§£æçš„é“¾å¼å¯¹è±¡
    """
    # å®šä¹‰å¯é€‰çš„è§’è‰²ï¼ˆåŒ…æ‹¬å®Œæˆä»»åŠ¡çš„é€‰é¡¹ï¼‰
    options = ["FINISH"] + members
    
    # å®šä¹‰è·¯ç”±å‡½æ•°çš„ç»“æ„
    function_def = {
        "name": "route",  # å‡½æ•°åç§°
        "description": "Select the next role.",  # å‡½æ•°æè¿°
        "parameters": {
            "title": "routeSchema",  # å‚æ•°æ ‡é¢˜
            "type": "object",  # å‚æ•°ç±»å‹
            "properties": {
                "next": {
                    "title": "Next",  # è·¯ç”±åˆ°ä¸‹ä¸€ä¸ªè§’è‰²çš„å‚æ•°
                    "anyOf": [
                        {"enum": options},  # å¯é€‰æ‹©çš„è§’è‰²é€‰é¡¹ï¼ŒåŒ…æ‹¬â€œFINISHâ€å’Œæ‰€æœ‰å›¢é˜Ÿæˆå‘˜
                    ],
                },
            },
            "required": ["next"],  # å¿…å¡«é¡¹ï¼šnext
        },
    }
    
    # åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿ï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºå’Œå›¢é˜Ÿæˆå‘˜ä¿¡æ¯
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),  # ç³»ç»Ÿæ¶ˆæ¯ï¼Œä¼ å…¥ç»™æ¨¡å‹çš„æŒ‡ä»¤
            
            MessagesPlaceholder(variable_name="messages"),  # å ä½ç¬¦ï¼Œç”¨äºæ’å…¥å®é™…çš„æ¶ˆæ¯å†…å®¹
            (
                "system",
                "Given the conversation above, who should act next?"
                " Or should we FINISH? Select one of: {options}",  # æç¤ºæ¨¡å‹é€‰æ‹©ä¸‹ä¸€ä¸ªæ‰§è¡Œä»»åŠ¡çš„è§’è‰²
            ),
        ]
    ).partial(options=str(options), team_members=", ".join(members))  # å°†è§’è‰²é€‰é¡¹å’Œæˆå‘˜ä¿¡æ¯ä¼ é€’åˆ°æ¨¡æ¿ä¸­
    
    # è¿”å›ä¸€ä¸ªç»„åˆäº†æç¤ºæ¨¡æ¿ã€å‡½æ•°ç»‘å®šå’Œè¾“å‡ºè§£æçš„å®Œæ•´ç®¡é“
    return (
        prompt
        | llm.bind_functions(functions=[function_def], function_call="route")  # ç»‘å®šå‡½æ•°å¹¶æŒ‡å®šè°ƒç”¨å‡½æ•°
        | JsonOutputFunctionsParser()  # è¾“å‡ºè§£æå™¨ï¼Œè§£æè¿”å›çš„ JSON æ ¼å¼æ•°æ®
    )


# åˆ›å»ºä¸€ä¸ª TavilySearchResults å·¥å…·å®ä¾‹ï¼Œç”¨äºæ‰§è¡Œæœç´¢æ“ä½œï¼Œè®¾ç½®æœ€å¤§è¿”å›ç»“æœä¸º 5
tavily_tool = TavilySearchResults(max_results=5)

# åˆ›å»ºç”¨äºæœç´¢çš„ä»£ç†
search_agent = create_react_agent(llm, tools=[tavily_tool])  # ä½¿ç”¨ TavilySearchResults å·¥å…·
search_node = functools.partial(agent_node, agent=search_agent, name="Search")

# åˆ›å»ºç”¨äºç½‘é¡µæŠ“å–çš„ä»£ç†
research_agent = create_react_agent(llm, tools=[scrape_webpages])  # ä½¿ç”¨ scrape_webpages å·¥å…·
research_node = functools.partial(agent_node, agent=research_agent, name="WebScraper")



# åˆ›å»ºæœ¬åœ°çŸ¥è¯†åº“ä»£ç†
local_knowledge_agent = create_react_agent(llm, tools=[local_knowledge_search])
local_knowledge_node = functools.partial(agent_node, agent=local_knowledge_agent, name="LocalKnowledgeBase")


supervisor_agent = create_team_supervisor(
    llm,
    "You are a supervisor tasked with managing a conversation between the"
    " following workers: LocalKnowledgeBase, Search, WebScraper. Always prioritize LocalKnowledgeBase."
    " Only if LocalKnowledgeBase cannot know the answer, you can choose Search or WebScraper."
    " Given the following user request, respond with the worker to act next."
    " Each worker will perform a task and respond with their results and status. When finished,"
    " respond with FINISH.",
    ["LocalKnowledgeBase", "WebScraper", "Search"],
)

# åˆ›å»ºå›¢é˜ŸçŠ¶æ€ç±»
class ResearchTeamState(TypedDict):
    messages: List[BaseMessage]
    team_members: List[str]
    next: str

# å®šä¹‰å›¢é˜Ÿå›¾
task_graph = StateGraph(ResearchTeamState)
task_graph.add_node("LocalKnowledgeBase", local_knowledge_node)
task_graph.add_node("WebScraper", research_node)
task_graph.add_node("Search", search_node)
task_graph.add_node("supervisor", supervisor_agent)

task_graph.add_edge("LocalKnowledgeBase", "supervisor")
task_graph.add_edge("WebScraper", "supervisor")
task_graph.add_edge("Search", "supervisor")

task_graph.add_conditional_edges(
    "supervisor",
    lambda x: x["next"],
    {"LocalKnowledgeBase":"LocalKnowledgeBase","WebScraper": "WebScraper", "Search": "Search", "FINISH": END},
)
task_graph.add_edge(START, "supervisor")
chain = task_graph.compile()

messages_key = "academic_messages"  # å”¯ä¸€é”®ï¼Œç¡®ä¿å„é¡µé¢ç‹¬ç«‹

# åˆå§‹åŒ–å½“å‰é¡µé¢çš„æ¶ˆæ¯è®°å½•
if messages_key not in st.session_state:
    st.session_state[messages_key] = []  # åˆå§‹åŒ–æ¶ˆ

# --- ä¸»èŠå¤©ç•Œé¢ ---
# st.header("Chat with GPT-4o")
# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state[messages_key]:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.markdown(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            st.markdown(message.content)

# è·å–ç”¨æˆ·è¾“å…¥
prompt = st.chat_input("Ask your question...")
if prompt:
    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    st.session_state[messages_key].append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    # è°ƒç”¨ LangGraph å¹¶æ˜¾ç¤ºç»“æœ
    with st.spinner("Finchat is thinking..."):
        initial_state = {"messages": st.session_state[messages_key]}
        response = chain.invoke(initial_state)

    # æ˜¾ç¤ºç»“æœ
    with st.chat_message("assistant"):
        st.markdown(response["messages"][-1].content)
        st.session_state[messages_key].append(AIMessage(content=response["messages"][-1].content))


      
#what does AIM mean?

# æ¸…ç©ºèŠå¤©è®°å½•æŒ‰é’®
if st.button("Clear Chat", key=f"clear_{messages_key}"):
    st.session_state[messages_key].clear()



with st.sidebar:
    st.image("/Users/alin/Documents/course_info/P1&P2/Financial Academic Knowledge.jpg")